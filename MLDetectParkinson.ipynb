{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b58fda5b",
   "metadata": {},
   "source": [
    "## Detecting Parkinson's Disease\n",
    "<p>Parkinsonâ€™s disease is a progressive disorder of the central nervous system affecting movement and inducing tremors and stiffness. It has 5 stages to it and affects more than 1 million individuals every year in India. This is chronic and has no cure yet. It is a neurodegenerative disorder affecting dopamine-producing neurons in the brain.</p>\n",
    "\n",
    "<p>I will be constructing this predictive model using XGBoost.  XGBoost stands for eXtreme Gradient Boosting and is based on decision trees.</p>\n",
    "\n",
    "<p>The data I will be using comes from the UCI Machine Learning Repository,  you can <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/\">download it here.</a> \n",
    "\n",
    "\n",
    "\n",
    "<p><img src=\"https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41598-020-78418-8/MediaObjects/41598_2020_78418_Fig1_HTML.png\" alt=\"Machine Learning for Parkinson's Disease Detection\"></p>\n",
    "\n",
    "\n",
    "\n",
    "## 1. Import the libraries and load data\n",
    "\n",
    "<p>I have stored the data locally so it is easy to load and I have previewed the head data.</p>\n",
    "\n",
    "<p>I am going to collect the labels and and features of the data, excluding the 'status' column.  Instead I'm going to count the labels within the 'status.' column.</p>\n",
    "\n",
    "<p>There are 147 ones and 48 zeros in the status column in our dataset.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f52125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             name  MDVP:Fo(Hz)  MDVP:Fhi(Hz)  MDVP:Flo(Hz)  MDVP:Jitter(%)  \\\n",
      "0  phon_R01_S01_1      119.992       157.302        74.997         0.00784   \n",
      "1  phon_R01_S01_2      122.400       148.650       113.819         0.00968   \n",
      "2  phon_R01_S01_3      116.682       131.111       111.555         0.01050   \n",
      "3  phon_R01_S01_4      116.676       137.871       111.366         0.00997   \n",
      "4  phon_R01_S01_5      116.014       141.781       110.655         0.01284   \n",
      "\n",
      "   MDVP:Jitter(Abs)  MDVP:RAP  MDVP:PPQ  Jitter:DDP  MDVP:Shimmer  ...  \\\n",
      "0           0.00007   0.00370   0.00554     0.01109       0.04374  ...   \n",
      "1           0.00008   0.00465   0.00696     0.01394       0.06134  ...   \n",
      "2           0.00009   0.00544   0.00781     0.01633       0.05233  ...   \n",
      "3           0.00009   0.00502   0.00698     0.01505       0.05492  ...   \n",
      "4           0.00011   0.00655   0.00908     0.01966       0.06425  ...   \n",
      "\n",
      "   Shimmer:DDA      NHR     HNR  status      RPDE       DFA   spread1  \\\n",
      "0      0.06545  0.02211  21.033       1  0.414783  0.815285 -4.813031   \n",
      "1      0.09403  0.01929  19.085       1  0.458359  0.819521 -4.075192   \n",
      "2      0.08270  0.01309  20.651       1  0.429895  0.825288 -4.443179   \n",
      "3      0.08771  0.01353  20.644       1  0.434969  0.819235 -4.117501   \n",
      "4      0.10470  0.01767  19.649       1  0.417356  0.823484 -3.747787   \n",
      "\n",
      "    spread2        D2       PPE  \n",
      "0  0.266482  2.301442  0.284654  \n",
      "1  0.335590  2.486855  0.368674  \n",
      "2  0.311173  2.342259  0.332634  \n",
      "3  0.334147  2.405554  0.368975  \n",
      "4  0.234513  2.332180  0.410335  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "147 48\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('parkinsons.data')\n",
    "print(df.head())\n",
    "\n",
    "# get features and labels\n",
    "features = df.loc[:,df.columns!='status'].values[:,1:]\n",
    "labels = df.loc[:,'status'].values\n",
    "\n",
    "# count status labels\n",
    "print(labels[labels==1].shape[0], labels[labels==0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95b59f",
   "metadata": {},
   "source": [
    "## 2. Initialise MinMaxScaler and Split Data\n",
    "<p>I will scale the features between -1 and 1 to normalise them. I will then use the function <code>fit_transform()</code> which fits to the data and then transforms it. It is a good habit to scale the data so that the algorithm will better fit the data. It is a rare case to get a higher accuracy without scaling.</p>\n",
    "\n",
    "<p>I have split the dataset into two parts 80% to train and 20% to test.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d8659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise features -1 to 1. \n",
    "scaler = MinMaxScaler((-1,1))\n",
    "x = scaler.fit_transform(features)\n",
    "y = labels\n",
    "\n",
    "# split the data set 20% test 80% training\n",
    "x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee9e7c",
   "metadata": {},
   "source": [
    "## 3. Initialise an XGBClassifier and train the model\n",
    "<p>This classifies using eXtreme Gradient Boosting - using gradient boosting algorithms for modern data science problems. It falls under the category of Ensemble Learning in ML, where we train and predict using many models to produce one superior output.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c36a0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:41:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=16, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise XGBClassifier and train the model\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6f374",
   "metadata": {},
   "source": [
    "## 4. Test the algorithm\n",
    "<p>Finally,  use the <code>y_pred</code> to predict on the test data and print the accuracy of the model. 94.8% accuracy is extremely high considering how many lines of code this particular program has.   \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c78b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.87179487179486\n"
     ]
    }
   ],
   "source": [
    "# use y_pred to analyse test data and print the models accuracy. \n",
    "y_pred = model.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred)*100)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
